{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fdcb1796-bc29-4562-ad5d-2d1875585d02",
      "metadata": {
        "id": "fdcb1796-bc29-4562-ad5d-2d1875585d02"
      },
      "source": [
        "# ðŸ§© Project Overview  \n",
        "\n",
        "## **Title**  \n",
        "**Discovering Hidden Barriers to Healthcare Access through Social Listening**\n",
        "\n",
        "---\n",
        "\n",
        "## **Goal**  \n",
        "Use social media conversations (Reddit: *r/BabyBumps, r/pregnant, r/pregnantover35, r/diabetes, r/diabetes_t1, r/diabetes_t2, r/GestationalDiabetes, r/AskDocs, r/HealthInsurance*) to identify hidden barriers that prevent people from seeking care â€” such as cost, distance, stigma, or bureaucracy â€” and explore whether those barriers appear to causally influence intent to seek vs. avoid care.\n",
        "\n",
        "---\n",
        "\n",
        "## **Value in Addressing**  \n",
        "Millions share candid health experiences online, yet data remains largely untapped for understanding real-world access challenges.  \n",
        "By combining unsupervised learning (topic modeling, clustering, and network analysis) with causal inference (estimating whether certain discussions precede shifts in health-seeking intent), this project aims to connect online discourse to public health insights that are scalable, reproducible, and ethically grounded.\n",
        "\n",
        "---\n",
        "\n",
        "## **Data Sources**  \n",
        "Reddit communities collected using the PRAW API:  \n",
        "*r/BabyBumps, r/pregnant, r/pregnantover35, r/diabetes, r/diabetes_t1, r/diabetes_t2, r/GestationalDiabetes, r/AskDocs, r/HealthInsurance*\n",
        "\n",
        "---\n",
        "\n",
        "### **Selected Subreddit Groups**\n",
        "\n",
        "| Subreddit Group           | Focus                             | Description of Group                                                                 |\n",
        "| -------------------------- | --------------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| **r/BabyBumps**           | General pregnancy journey         | Community for expectant mothers sharing experiences, emotional narratives, and challenges related to pregnancy care and prenatal access. |\n",
        "| **r/pregnant**            | Early-stage pregnancy experiences | Discussion forum covering early pregnancy, first-time parent concerns, high-risk pregnancies, and barriers to accessing prenatal healthcare. |\n",
        "| **r/pregnantover35**      | Advanced maternal age pregnancy   | Support and discussion group for individuals 35+ navigating pregnancy, fertility, and higher-risk maternal care access challenges. |\n",
        "| **r/GestationalDiabetes** | Pregnancy-specific diabetes       | Focused on individuals managing gestational diabetes, highlighting the intersection between maternal health and chronic disease management. |\n",
        "| **r/diabetes**            | General diabetes management       | Broad community addressing chronic disease care, treatment costs, insurance barriers, and ongoing access to healthcare resources. |\n",
        "| **r/diabetes_t1**         | Type 1 diabetes experiences       | Specialized forum emphasizing the experiences of those with Type 1 diabetes, including insulin affordability, medical technology, and self-management routines. |\n",
        "| **r/diabetes_t2**         | Type 2 diabetes experiences       | Community focused on lifestyle management, medication access, affordability, and comorbidities associated with Type 2 diabetes. |\n",
        "| **r/AskDocs**             | Direct health-seeking behavior    | Platform where users seek medical guidance from licensed healthcare professionals, providing insight into real-world intent to seek versus avoid care. |\n",
        "| **r/HealthInsurance**     | Policy and access navigation      | Community focused on understanding and navigating insurance coverage, denials, affordability, and systemic healthcare access challenges. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Supplementary Public Health Datasets (for optional validation)**  \n",
        "- CDC Behavioral Risk Factor Surveillance System (BRFSS)  \n",
        "- HRSA Health Resources Data  \n",
        "- Kaiser Family Foundation (KFF) Health Access Indicators  \n",
        "\n",
        "These datasets may be used to validate observed trends (e.g., cost or insurance-related barriers) against regional or national health access metrics.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Analytic Approach**  \n",
        "- Unsupervised Learning: Topic modeling, clustering, and barrier co-occurrence network mapping  \n",
        "- Temporal Trend & Causal Inference: Granger analysis on intent shifts  \n",
        "- Outcome Focus: Identifying causal relationships between language patterns and healthcare-seeking behavior\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Questions to Answer**\n",
        "1. Which barriers are most frequently cited as reasons for avoiding or delaying care?  \n",
        "2. Do certain barriers cluster together (e.g., stigma + cost)?  \n",
        "3. Which linguistic patterns signal implied causality in patient narratives, and how do these patterns align with observed temporal shifts in intent and behavior?\n",
        "4. Are there hidden or unreported barriers (e.g., partner/family pressure, work schedule conflicts)?  \n",
        "5. Do different communities (e.g., diabetes vs. pregnancy) experience distinct access challenges?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F_HJqt56pp7l",
      "metadata": {
        "id": "F_HJqt56pp7l"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PkkrJx0-5PNP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkkrJx0-5PNP",
        "outputId": "aa352c03-e1a5-4fd9-8e2f-ad5d1e679381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.12/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.12/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30977d8b-a12d-4ad0-adf3-62d0fb2a53d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30977d8b-a12d-4ad0-adf3-62d0fb2a53d4",
        "outputId": "a449cd68-069b-438c-f915-665d891275ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read-only mode: True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=userdata.get(\"CLIENT_ID\"),\n",
        "    client_secret=userdata.get(\"CLIENT_SECRET\"),\n",
        "    user_agent=userdata.get(\"USER_AGENT\"),\n",
        "    check_for_async=False,\n",
        ")\n",
        "\n",
        "print(\"Read-only mode:\", reddit.read_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665bc151-110e-4ae1-a4e8-646b9d9e62ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "665bc151-110e-4ae1-a4e8-646b9d9e62ca",
        "outputId": "94f12091-2ddb-41fa-8120-8b74aa1aea63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting from r/BabyBumps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-553858498.py:20: DeprecationWarning:\n",
            "\n",
            "datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting from r/pregnant...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "#Add Subreddit groups to pull\n",
        "subreddits = [\"BabyBumps\", \"pregnant\", \"diabetes\", \"GestationalDiabetes\", \"diabetes_t1\", \"diabetes_t2\", \"pregnantover35\", \"AskDocs\", \"HealthInsurance\"]\n",
        "all_posts = []\n",
        "\n",
        "for sub in subreddits:\n",
        "    print(f\"Collecting from r/{sub}...\")\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for post in subreddit.top(time_filter=\"year\", limit=7000):\n",
        "        if post.selftext in [\"[removed]\", \"[deleted]\", None, \"\"]:\n",
        "            continue\n",
        "        all_posts.append({\n",
        "            \"subreddit\": sub,\n",
        "            \"id\": post.id,\n",
        "            \"title\": post.title,\n",
        "            \"text\": post.selftext,\n",
        "            \"created_utc\": post.created_utc,\n",
        "            \"created\": datetime.utcfromtimestamp(post.created_utc)\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2964f36-40be-4043-8223-ed7959cb138a",
      "metadata": {
        "id": "d2964f36-40be-4043-8223-ed7959cb138a"
      },
      "outputs": [],
      "source": [
        "#Create dataframe and print\n",
        "df = pd.DataFrame(all_posts)\n",
        "print(df[\"subreddit\"].value_counts())\n",
        "df.to_parquet(\"reddit_raw.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ZXKOSq0p930",
      "metadata": {
        "id": "3ZXKOSq0p930"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "656986d9-b443-4548-9b76-d7371dc05da9",
      "metadata": {
        "id": "656986d9-b443-4548-9b76-d7371dc05da9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text) if text is not None else \"\"\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)        # remove URLs\n",
        "    text = re.sub(r\"[\\r\\n]+\", \" \", text)        # remove newlines\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)         # collapse spaces\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Combine title + body\n",
        "df[\"text_full\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).str.strip()\n",
        "\n",
        "# Create cleaned text\n",
        "df[\"text_clean\"] = df[\"text_full\"].apply(clean_text)\n",
        "\n",
        "# Optional: drop very short posts\n",
        "df = df[df[\"text_clean\"].str.len() > 50].copy()\n",
        "\n",
        "print(df.columns)\n",
        "df[[\"subreddit\", \"text_clean\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YOw8ED03qMoX",
      "metadata": {
        "id": "YOw8ED03qMoX"
      },
      "source": [
        "# Barrier & Intent Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c0afe0-ad2f-42d1-8a18-b78e12e0b626",
      "metadata": {
        "id": "43c0afe0-ad2f-42d1-8a18-b78e12e0b626"
      },
      "outputs": [],
      "source": [
        "# Barrier keyword dictionaries\n",
        "barriers = {\n",
        "    \"cost\": [\n",
        "        \"can't pay\", \"cannot afford\", \"couldn't afford\",\n",
        "        \"insurance denied\", \"insurance won't cover\",\n",
        "        \"copay\", \"co-pay\",\n",
        "        \"bills\", \"medical bills\",\n",
        "        \"coverage\", \"lost my coverage\",\n",
        "        \"expensive\", \"too expensive\", \"too costly\",\n",
        "        \"price\", \"costs too much\",\n",
        "        \"out of pocket\", \"pay out of pocket\",\n",
        "        \"deductible\", \"high deductible\",\n",
        "        \"can't afford strips\", \"can't afford insulin\", \"can't afford cgm\",\n",
        "        \"budget\", \"financial\", \"money\",\n",
        "        \"unaffordable\", \"no insurance\", \"uninsured\",\n",
        "        \"out of network\",\n",
        "        \"medical debt\", \"financially struggling\",\n",
        "        \"prescription too expensive\"\n",
        "    ],\n",
        "\n",
        "    \"geo\": [\n",
        "        \"too far to travel\",\n",
        "        \"no transport\", \"no transportation\", \"don't have transportation\",\n",
        "        \"transportation issues\",\n",
        "        \"can't drive\", \"no ride\", \"no car\",\n",
        "        \"miles away\", \"far away\", \"hours away\",\n",
        "        \"long drive\", \"drive hours\",\n",
        "        \"can't get there\", \"can't get to the clinic\",\n",
        "        \"clinic too far\", \"clinic is too far\",\n",
        "        \"nearest doctor is far\", \"nearest is\", \"closest clinic\",\n",
        "        \"rural\", \"rural area\",\n",
        "        \"distance to hospital\",\n",
        "        \"travel time\", \"travel costs\",\n",
        "        \"don't live near\", \"not available in my area\"\n",
        "    ],\n",
        "\n",
        "    \"bureau\": [\n",
        "        \"paperwork\", \"paper work\",\n",
        "        \"waitlist\", \"waiting list\",\n",
        "        \"appointment backlog\",\n",
        "        \"denied\", \"denied referral\",\n",
        "        \"referral\", \"can't get a referral\",\n",
        "        \"authorization\", \"prior authorization\",\n",
        "        \"red tape\",\n",
        "        \"can't get through\", \"on hold\",\n",
        "        \"weeks wait\", \"months wait\", \"appointment is months out\",\n",
        "        \"couldn't get in\", \"no availability\", \"booked out\",\n",
        "        \"can't get an appointment\", \"unable to schedule\",\n",
        "        \"impossible to get\",\n",
        "        \"gave up calling\",\n",
        "        \"got lost in the system\",\n",
        "        \"confusing process\",\n",
        "        \"don't know where to go\",\n",
        "        \"can't find provider\",\n",
        "        \"system won't help\"\n",
        "    ],\n",
        "\n",
        "    \"stigma\": [\n",
        "        \"stigma\", \"fear of stigma\",\n",
        "        \"embarrassed\", \"embarrassing\",\n",
        "        \"ashamed\", \"ashamed to ask\",\n",
        "        \"shame\", \"guilty\", \"blamed myself\", \"feel like a failure\",\n",
        "        \"feel judged\", \"feeling judged\",\n",
        "        \"afraid to ask\",\n",
        "        \"don't want people to know\", \"hiding it\", \"keeping it secret\",\n",
        "        \"scared to tell\",\n",
        "        \"family doesn't understand\", \"family does not understand\",\n",
        "        \"partner thinks\", \"people will think\",\n",
        "        \"privacy concerns\",\n",
        "        \"afraid to talk about it\",\n",
        "        \"worried about discrimination\",\n",
        "        \"scared of what doctor will say\"\n",
        "    ],\n",
        "\n",
        "    \"language_info\": [\n",
        "        \"language barrier\",\n",
        "        \"don't understand instructions\",\n",
        "        \"don't understand what they said\",\n",
        "        \"too much medical jargon\",\n",
        "        \"can't read paperwork\",\n",
        "        \"can't read the forms\",\n",
        "        \"not explained clearly\",\n",
        "        \"hard to follow directions\",\n",
        "        \"confusing information\",\n",
        "        \"translation issue\", \"interpreter needed\",\n",
        "        \"no materials in my language\"\n",
        "    ],\n",
        "\n",
        "    \"time_work\": [\n",
        "        \"can't get time off\",\n",
        "        \"can't take time off\",\n",
        "        \"work won't let me\",\n",
        "        \"no sick leave\",\n",
        "        \"no sick days\",\n",
        "        \"clinic not open when i can go\",\n",
        "        \"clinic not open when I can go\",\n",
        "        \"too busy for appointment\",\n",
        "        \"too busy to see a doctor\",\n",
        "        \"have to work during hours\",\n",
        "        \"conflicts with work\",\n",
        "        \"can't schedule around job\",\n",
        "        \"work schedule conflict\",\n",
        "        \"shift work makes it hard\",\n",
        "        \"no time for appointment\"\n",
        "    ],\n",
        "\n",
        "    \"trust_mistrust\": [\n",
        "        \"don't trust doctor\", \"do not trust doctor\",\n",
        "        \"don't trust hospital\", \"do not trust hospital\",\n",
        "        \"don't trust healthcare\", \"do not trust healthcare\",\n",
        "        \"don't trust medical advice\",\n",
        "        \"lost faith in system\",\n",
        "        \"healthcare failed me\",\n",
        "        \"bad experience before\",\n",
        "        \"had a bad experience\",\n",
        "        \"wasn't listened to\", \"was not listened to\",\n",
        "        \"doctor ignored me\",\n",
        "        \"didn't feel heard\", \"did not feel heard\",\n",
        "        \"they didn't believe me\",\n",
        "        \"they don't believe me\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Care trajectory / intent keywords\n",
        "\n",
        "barrier_overcome = [\n",
        "    \"finally got\", \"was able to\", \"managed to get\",\n",
        "    \"insurance approved\", \"found a way\", \"worth it in the end\",\n",
        "    \"glad I went\", \"should have gone sooner\", \"wish I hadn't waited\"\n",
        "]\n",
        "\n",
        "intent_seek = [\n",
        "    \"booked an appointment\", \"booked appointment\",\n",
        "    \"made an appointment\", \"have an appointment\",\n",
        "    \"seeing my doctor\", \"seeing the doctor\",\n",
        "    \"scheduled a visit\", \"going next week\",\n",
        "    \"going to the doctor\", \"finally scheduled\",\n",
        "    \"got an appointment\", \"set up appointment\",\n",
        "    \"went to see\", \"visited the doctor\",\n",
        "    \"started seeing\",\n",
        "    \"got referred\", \"making an appointment\",\n",
        "    \"decided to go\"\n",
        "]\n",
        "\n",
        "intent_considering = [\n",
        "    \"worth it to see\",\n",
        "    \"considering\", \"considering going\",\n",
        "    \"should I\", \"should i\",\n",
        "    \"is it necessary\",\n",
        "    \"thinking about\", \"thinking about going\",\n",
        "    \"wondering if I should\", \"wondering if i should\",\n",
        "    \"debating whether\"\n",
        "]\n",
        "\n",
        "intent_delayed = [\n",
        "    \"bit the bullet\",\n",
        "    \"eventually\",\n",
        "    \"putting off\",\n",
        "    \"finally went\",\n",
        "    \"took me weeks\",\n",
        "    \"waited too long\",\n",
        "    \"delayed\"\n",
        "]\n",
        "\n",
        "intent_ambivalent = [\n",
        "    \"not sure if\",\n",
        "    \"don't know if I should\", \"don't know if i should\",\n",
        "    \"on the fence\",\n",
        "    \"hesitant to\",\n",
        "    \"reluctant to\"\n",
        "]\n",
        "\n",
        "intent_avoid = [\n",
        "    \"not going to the doctor\", \"not going\",\n",
        "    \"avoiding the doctor\", \"avoiding doctors\",\n",
        "    \"not worth it\",\n",
        "    \"won't go\", \"will not go\",\n",
        "    \"cant go\", \"can't go\",\n",
        "    \"skip the appointment\", \"skipping\",\n",
        "    \"skipped appointment\",\n",
        "    \"cancelled appointment\", \"canceled appointment\",\n",
        "    \"rather not\",\n",
        "    \"didn't go\", \"did not go\",\n",
        "    \"not bothering\",\n",
        "    \"gave up trying\",\n",
        "    \"too much hassle\",\n",
        "    \"managing on my own\",\n",
        "    \"doing it myself\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0eb46d1-d140-4c11-9dfe-085224abd621",
      "metadata": {
        "id": "f0eb46d1-d140-4c11-9dfe-085224abd621"
      },
      "outputs": [],
      "source": [
        "# Tagging functions\n",
        "\n",
        "def tag_barriers(txt):\n",
        "    \"\"\"\n",
        "    Return a list of barrier categories present in the text.\n",
        "    Uses simple substring matching; each barrier type is only tagged once.\n",
        "    \"\"\"\n",
        "    txt = txt.lower()\n",
        "    found = []\n",
        "    for k, words in barriers.items():\n",
        "        # Sort longest phrases first to favor more specific matches\n",
        "        for w in sorted(words, key=len, reverse=True):\n",
        "            if w.lower() in txt:\n",
        "                found.append(k)\n",
        "                break  # Only flag each barrier type once\n",
        "    # Remove duplicates just in case\n",
        "    return list(set(found))\n",
        "\n",
        "\n",
        "def tag_intent(txt):\n",
        "    \"\"\"\n",
        "    Return a single intent label based on priority:\n",
        "    avoid > delayed > ambivalent > considering > seek\n",
        "    \"\"\"\n",
        "    txt = txt.lower()\n",
        "\n",
        "    for w in sorted(intent_avoid, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return \"avoid\"\n",
        "\n",
        "    for w in sorted(intent_delayed, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return \"delayed\"\n",
        "\n",
        "    for w in sorted(intent_ambivalent, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return \"ambivalent\"\n",
        "\n",
        "    for w in sorted(intent_considering, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return \"considering\"\n",
        "\n",
        "    for w in sorted(intent_seek, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return \"seek\"\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def tag_barrier_overcome(txt):\n",
        "    \"\"\"\n",
        "    Return True if any 'barrier_overcome' phrase is present.\n",
        "    \"\"\"\n",
        "    txt = txt.lower()\n",
        "    for w in sorted(barrier_overcome, key=len, reverse=True):\n",
        "        if w.lower() in txt:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Apply tagging to the dataframe\n",
        "\n",
        "print(\"Tagging barriers...\")\n",
        "df[\"barriers\"] = df[\"text_clean\"].apply(tag_barriers)\n",
        "\n",
        "print(\"Tagging intent...\")\n",
        "df[\"intent\"] = df[\"text_clean\"].apply(tag_intent)\n",
        "\n",
        "print(\"Tagging barrier_overcome...\")\n",
        "df[\"barrier_overcome\"] = df[\"text_clean\"].apply(tag_barrier_overcome)\n",
        "\n",
        "# --- Create barrier flags (one per category + overall) ---\n",
        "\n",
        "df[\"has_cost\"]           = df[\"barriers\"].apply(lambda x: \"cost\" in x)\n",
        "df[\"has_geo\"]            = df[\"barriers\"].apply(lambda x: \"geo\" in x)\n",
        "df[\"has_bureau\"]         = df[\"barriers\"].apply(lambda x: \"bureau\" in x)\n",
        "df[\"has_stigma\"]         = df[\"barriers\"].apply(lambda x: \"stigma\" in x)\n",
        "df[\"has_language_info\"]  = df[\"barriers\"].apply(lambda x: \"language_info\" in x)\n",
        "df[\"has_time_work\"]      = df[\"barriers\"].apply(lambda x: \"time_work\" in x)\n",
        "df[\"has_trust_mistrust\"] = df[\"barriers\"].apply(lambda x: \"trust_mistrust\" in x)\n",
        "\n",
        "df[\"has_any_barrier\"]    = df[\"barriers\"].apply(lambda x: len(x) > 0)\n",
        "df[\"barrier_count\"]      = df[\"barriers\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bACvtgHNqpdk",
      "metadata": {
        "id": "bACvtgHNqpdk"
      },
      "source": [
        "# Barrier & Intent Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9441eab3-1f67-4dba-b728-492713a530eb",
      "metadata": {
        "id": "9441eab3-1f67-4dba-b728-492713a530eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Analyze and create comprehensive reporting\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BARRIER OVERLAP ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Posts with ONLY a single barrier type\n",
        "only_cost = df[\"barriers\"].apply(lambda x: (len(x) == 1) and (\"cost\" in x))\n",
        "only_bureau = df[\"barriers\"].apply(lambda x: (len(x) == 1) and (\"bureau\" in x))\n",
        "\n",
        "print(f\"\\nPosts with ONLY cost barriers: {only_cost.sum()}\")\n",
        "print(f\"Posts with ONLY bureau barriers: {only_bureau.sum()}\")\n",
        "\n",
        "# Posts with BOTH cost AND bureau (regardless of other barriers)\n",
        "both_cost_bureau = df[\"barriers\"].apply(lambda x: (\"cost\" in x) and (\"bureau\" in x))\n",
        "print(f\"Posts with BOTH cost AND bureau barriers: {both_cost_bureau.sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"OVERALL STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nTotal posts: {len(df)}\")\n",
        "print(f\"Posts with at least one barrier: {df['has_any_barrier'].sum()}\")\n",
        "print(f\"Posts with no barriers: {(~df['has_any_barrier']).sum()}\")\n",
        "print(f\"Posts with multiple barriers: {df['barriers'].apply(lambda x: len(x) > 1).sum()}\")\n",
        "print(f\"Average barriers per post: {df['barriers'].apply(len).mean():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BARRIER TYPE COUNTS\")\n",
        "print(\"=\" * 50)\n",
        "print(df[\"barriers\"].explode().value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"INTENT COUNTS\")\n",
        "print(\"=\" * 50)\n",
        "print(df[\"intent\"].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BARRIER + INTENT CROSSTAB\")\n",
        "print(\"=\" * 50)\n",
        "print(pd.crosstab(df['has_any_barrier'], df['intent'], dropna=False, margins=True))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BARRIER COMBINATIONS (Top 10)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Use all has_* barrier flags except has_any_barrier\n",
        "barrier_flag_cols = [\n",
        "    col for col in df.columns\n",
        "    if col.startswith(\"has_\") and col not in [\"has_any_barrier\"]\n",
        "]\n",
        "\n",
        "barrier_combos = df.groupby(barrier_flag_cols).size()\n",
        "print(barrier_combos.sort_values(ascending=False).head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CAUSAL INDICATORS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if df['has_any_barrier'].sum() > 0:\n",
        "    avoid_with_barrier = df[df['has_any_barrier'] & (df['intent'] == 'avoid')].shape[0]\n",
        "    total_with_barrier = df['has_any_barrier'].sum()\n",
        "    print(f\"Avoid rate WITH barriers: {avoid_with_barrier / total_with_barrier * 100:.1f}%\")\n",
        "\n",
        "if (~df['has_any_barrier']).sum() > 0:\n",
        "    avoid_without_barrier = df[~df['has_any_barrier'] & (df['intent'] == 'avoid')].shape[0]\n",
        "    total_without_barrier = (~df['has_any_barrier']).sum()\n",
        "    print(f\"Avoid rate WITHOUT barriers: {avoid_without_barrier / total_without_barrier * 100:.1f}%\")\n",
        "\n",
        "# Save tagged data\n",
        "df.to_parquet(\"reddit_tagged.parquet\", index=False)\n",
        "print(\"\\nâœ… Data saved to reddit_tagged.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e237d20-671b-42ac-988e-d00f96fd2119",
      "metadata": {
        "id": "6e237d20-671b-42ac-988e-d00f96fd2119"
      },
      "outputs": [],
      "source": [
        "# Run analysis to analyze posts by barrier type\n",
        "\n",
        "barrier_types = [\n",
        "    \"cost\",\n",
        "    \"geo\",\n",
        "    \"bureau\",\n",
        "    \"stigma\",\n",
        "    \"language_info\",\n",
        "    \"time_work\",\n",
        "    \"trust_mistrust\"\n",
        "]\n",
        "\n",
        "for barrier_type in barrier_types:\n",
        "    flag_col = f\"has_{barrier_type}\"\n",
        "    if flag_col not in df.columns:\n",
        "        # Skip gracefully if a flag wasn't created for some reason\n",
        "        continue\n",
        "\n",
        "    subset = df[df[flag_col]]\n",
        "\n",
        "    print(f\"\\n{barrier_type.upper()} barrier breakdown:\")\n",
        "    print(f\"  Total posts: {len(subset)}\")\n",
        "\n",
        "    with_intent = subset[\"intent\"].notna().sum()\n",
        "    print(f\"  With intent: {with_intent}\")\n",
        "\n",
        "    if with_intent > 0:\n",
        "        print(subset[\"intent\"].value_counts(dropna=False))\n",
        "\n",
        "    if len(subset) > 0:\n",
        "        avoid_rate = (subset[\"intent\"] == \"avoid\").sum() / len(subset) * 100\n",
        "        print(f\"  Avoid rate: {avoid_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3XsbluYmq3kC",
      "metadata": {
        "id": "3XsbluYmq3kC"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qnBBIdBm82y0",
      "metadata": {
        "id": "qnBBIdBm82y0"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114abfac-da83-4c21-ae6a-2be8c4800279",
      "metadata": {
        "id": "114abfac-da83-4c21-ae6a-2be8c4800279"
      },
      "outputs": [],
      "source": [
        "#Run sentiment analysis using Vader\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create Sentiment column\n",
        "df[\"sentiment_score\"] = df[\"text\"].fillna(\"\").apply(\n",
        "    lambda x: sid.polarity_scores(str(x))[\"compound\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b224832-d721-4578-8803-2d152194ae22",
      "metadata": {
        "id": "1b224832-d721-4578-8803-2d152194ae22"
      },
      "outputs": [],
      "source": [
        "# Sentiment distribution\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SENTIMENT DISTRIBUTION\")\n",
        "print(\"=\" * 50)\n",
        "print(df[\"sentiment_score\"].describe())\n",
        "\n",
        "# Sentiment by label\n",
        "def label_sentiment(score):\n",
        "    if score >= 0.05:\n",
        "        return \"positive\"\n",
        "    elif score <= -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(label_sentiment)\n",
        "print(df[\"sentiment_label\"].value_counts())\n",
        "\n",
        "# Average sentiment by barrier type\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"AVERAGE SENTIMENT BY BARRIER\")\n",
        "print(\"=\" * 50)\n",
        "for barrier in [\"cost\", \"geo\", \"bureau\", \"stigma\",\n",
        "                \"language_info\", \"time_work\", \"trust_mistrust\"]:\n",
        "    mask = df[f\"has_{barrier}\"]\n",
        "    if mask.any():\n",
        "        mean_score = df.loc[mask, \"sentiment_score\"].mean()\n",
        "        print(f\"{barrier:15}: {mean_score:6.3f}  (n={mask.sum()})\")\n",
        "\n",
        "# Average sentiment by intent\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"AVERAGE SENTIMENT BY INTENT\")\n",
        "print(\"=\" * 50)\n",
        "print(df.groupby(\"intent\")[\"sentiment_score\"].mean().sort_values())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JNXOhqTIrGmB",
      "metadata": {
        "id": "JNXOhqTIrGmB"
      },
      "source": [
        "# Unsupervised Models (LDA & BERTopic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27c4aec-9748-4e8d-a430-a14088e4a475",
      "metadata": {
        "id": "f27c4aec-9748-4e8d-a430-a14088e4a475"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Vectorize text\n",
        "texts = df[\"text_clean\"].fillna(\"\")\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.95,        # ignore very common words\n",
        "    min_df=10,          # ignore very rare words\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "dtm = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Fit LDA\n",
        "n_topics = 10\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    random_state=42,\n",
        "    learning_method=\"batch\"\n",
        ")\n",
        "lda_doc_topic = lda.fit_transform(dtm)\n",
        "\n",
        "# Assign dominant LDA topic per post\n",
        "df[\"lda_topic\"] = lda_doc_topic.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_HiHfjzU-Yba",
      "metadata": {
        "id": "_HiHfjzU-Yba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Top LDA topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "def print_lda_topics(model, feature_names, n_top_words=10):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_indices = topic.argsort()[::-1][:n_top_words]\n",
        "        top_words = [feature_names[i] for i in top_indices]\n",
        "        print(f\"\\nTopic {topic_idx}:\")\n",
        "        print(\", \".join(top_words))\n",
        "\n",
        "print_lda_topics(lda, feature_names, n_top_words=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LWNWyyyZ-yvw",
      "metadata": {
        "id": "LWNWyyyZ-yvw"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic[visualization] umap-learn hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mF24_KVB-yhA",
      "metadata": {
        "id": "mF24_KVB-yhA"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "print(\"BERTopic imported successfully âœ…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r_atpDAoA4f5",
      "metadata": {
        "id": "r_atpDAoA4f5"
      },
      "outputs": [],
      "source": [
        "# Initialize and fit the model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
        "\n",
        "# Fit to your cleaned text column (replace with your actual text column)\n",
        "topics, probs = topic_model.fit_transform(df[\"text_clean\"].dropna())\n",
        "\n",
        "# Add results back to dataframe for analysis\n",
        "df[\"topic\"] = topics\n",
        "df[\"topic_probability\"] = probs.max(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7df9c7f-ad1a-42e4-9e9f-dff5b32bc40a",
      "metadata": {
        "id": "f7df9c7f-ad1a-42e4-9e9f-dff5b32bc40a"
      },
      "outputs": [],
      "source": [
        "#BERTopic analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BERTopic Summary\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Total topics (excluding -1 \"outliers\")\n",
        "num_topics = len(set(df[\"topic\"])) - (1 if -1 in df[\"topic\"].unique() else 0)\n",
        "print(f\"Total topics (excluding outliers): {num_topics}\")\n",
        "print(f\"Outlier posts: {(df['topic'] == -1).sum()}\")\n",
        "\n",
        "# Top 10 most frequent topics\n",
        "print(\"\\nTop 10 topics by frequency:\")\n",
        "print(df[\"topic\"].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1NgSn7AU-x",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ca1NgSn7AU-x"
      },
      "outputs": [],
      "source": [
        "#Parameter tune for better output\n",
        "topic_model = BERTopic(\n",
        "    language=\"english\",\n",
        "    min_topic_size=20,      # smaller = more granular clusters\n",
        "    nr_topics=None,         # keep dynamic, or set an upper limit\n",
        "    n_gram_range=(1, 2),    # include bigrams\n",
        "    calculate_probabilities=True\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(df[\"text_clean\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YZ20L0_0rvC7",
      "metadata": {
        "id": "YZ20L0_0rvC7"
      },
      "source": [
        "# Integrated Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TgoH1uxjAZjg",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TgoH1uxjAZjg"
      },
      "outputs": [],
      "source": [
        "# Get top keywords for each topic\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info.head(10))\n",
        "\n",
        "# Display keywords for a specific topic (e.g., topic 0)\n",
        "topic_id = 0\n",
        "print(f\"\\nTop words for topic {topic_id}:\")\n",
        "print(topic_model.get_topic(topic_id))\n",
        "\n",
        "# Show a few representative posts from that topic\n",
        "sample_posts = df[df[\"topic\"] == topic_id].sample(5)\n",
        "print(sample_posts[[\"subreddit\", \"intent\", \"barriers\", \"sentiment_label\", \"text_clean\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U9TXiDsn_5QW",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U9TXiDsn_5QW"
      },
      "outputs": [],
      "source": [
        "#Compare LDA with BERTopic + barriers/intent\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\nLDA topic counts:\")\n",
        "print(df[\"lda_topic\"].value_counts().head())\n",
        "\n",
        "print(\"\\nLDA topic Ã— BERTopic topic (small sample):\")\n",
        "print(pd.crosstab(df[\"lda_topic\"], df[\"topic\"]).head())\n",
        "\n",
        "print(\"\\nAverage sentiment per LDA topic:\")\n",
        "print(df.groupby(\"lda_topic\")[\"sentiment_score\"].mean().sort_values())\n",
        "\n",
        "print(\"\\nAverage cost barrier presence per LDA topic:\")\n",
        "print(df.groupby(\"lda_topic\")[\"has_cost\"].mean().sort_values(ascending=False).head())\n",
        "\n",
        "print(\"\\nIntent by LDA topic:\")\n",
        "print(pd.crosstab(df[\"lda_topic\"], df[\"intent\"], normalize=\"index\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pWtliwcvAEEh",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pWtliwcvAEEh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Topic word embeddings coherence\n",
        "topics_dict = topic_model.get_topics()\n",
        "coherence_scores = {}\n",
        "for t, words in topics_dict.items():\n",
        "    if t == -1:  # Skip outliers\n",
        "        continue\n",
        "    embeddings = [topic_model.embedding_model.embed(word) for word, _ in words[:10]]\n",
        "    sims = cosine_similarity(embeddings)\n",
        "    coherence_scores[t] = np.mean(sims[np.triu_indices_from(sims, 1)])\n",
        "\n",
        "print(\"\\nApproximate topic coherence (top 10):\")\n",
        "print(sorted(coherence_scores.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5HBHvsGn__vw",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5HBHvsGn__vw"
      },
      "outputs": [],
      "source": [
        "#Barrier & Intent Crossover by Topic\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TOPIC vs BARRIER / INTENT RELATIONSHIP\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Barrier overlap by topic\n",
        "for barrier in [\"cost\", \"geo\", \"bureau\", \"stigma\",\n",
        "                \"language_info\", \"time_work\", \"trust_mistrust\"]:\n",
        "    col = f\"has_{barrier}\"\n",
        "    if col in df.columns:\n",
        "        mean_val = df.groupby(\"topic\")[col].mean().rename(f\"{barrier}_rate\")\n",
        "        print(f\"\\nAverage {barrier} presence by topic:\")\n",
        "        print(mean_val.sort_values(ascending=False).head(10))\n",
        "\n",
        "# Intent breakdown by topic\n",
        "intent_ct = pd.crosstab(df[\"topic\"], df[\"intent\"], normalize=\"index\")\n",
        "print(\"\\nIntent distribution by topic:\")\n",
        "print(intent_ct.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z64PTmxWDPBf",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z64PTmxWDPBf"
      },
      "outputs": [],
      "source": [
        "#Sentiment Relationship by Topic\n",
        "topic_sentiment = (\n",
        "    df.groupby(\"topic\")[\"sentiment_score\"]\n",
        "      .agg([\"mean\", \"count\"])\n",
        "      .sort_values(\"mean\")\n",
        ")\n",
        "print(topic_sentiment.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JM53aQjgsGgl",
      "metadata": {
        "id": "JM53aQjgsGgl"
      },
      "source": [
        "# Visualizations & Interpretations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dc8XCV_iuZWW",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dc8XCV_iuZWW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figures directory\n",
        "os.makedirs(\"figures\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_be0AvVUYKQg",
      "metadata": {
        "id": "_be0AvVUYKQg"
      },
      "outputs": [],
      "source": [
        "# From stats printout:\n",
        "barriers = [\"cost\", \"stigma\", \"bureau\", \"geo\"]\n",
        "counts   = [240, 89, 88, 32]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "bars = plt.bar(barriers, counts, color=['#cc0000','#880000','#550000','#333333'])\n",
        "\n",
        "# formatting\n",
        "plt.title(\"Barrier Type Counts\")\n",
        "plt.ylabel(\"Number of Posts\")\n",
        "plt.xlabel(\"Barrier Type\")\n",
        "\n",
        "# Add labels above bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 3,\n",
        "             '%d' % int(height), ha='center')\n",
        "\n",
        "# Light horizontal grid\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvGlrMfCvR3X",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UvGlrMfCvR3X"
      },
      "outputs": [],
      "source": [
        "# Topic and Barrier Trends over Time\n",
        "df['month'] = df['created'].dt.to_period('M').astype(str)\n",
        "barrier_trends = (\n",
        "    df.groupby(['subreddit', 'month'])\n",
        "      .agg(barrier_rate=('has_any_barrier', 'mean'),\n",
        "           cost_rate=('has_cost', 'mean'),\n",
        "           stigma_rate=('has_stigma', 'mean'),\n",
        "           n_posts=('id', 'count'))\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(data=barrier_trends, x='month', y='barrier_rate', hue='subreddit')\n",
        "plt.title(\"Barrier Mentions Over Time by Community\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/barrier_trends_by_community.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4A858Pkuuc5e",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4A858Pkuuc5e"
      },
      "outputs": [],
      "source": [
        "# Sentiment vs Intent\n",
        "sent_intent = (\n",
        "    df.groupby('intent')['sentiment_score']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        "      .sort_values('sentiment_score')\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=sent_intent, x='intent', y='sentiment_score', palette='coolwarm')\n",
        "plt.title(\"Average Sentiment by Health-Seeking Intent\")\n",
        "plt.ylabel(\"Mean Sentiment (VADER Compound)\")\n",
        "plt.xlabel(\"Intent Type\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/sentiment_by_intent.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s2Qd90Hvu2yX",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s2Qd90Hvu2yX"
      },
      "outputs": [],
      "source": [
        "# Heatmap of sentiment x barrier type\n",
        "heatmap_data = {}\n",
        "for barrier in [\"cost\", \"geo\", \"bureau\", \"stigma\", \"language_info\", \"time_work\", \"trust_mistrust\"]:\n",
        "    mask = df[f\"has_{barrier}\"]\n",
        "    if mask.any():\n",
        "        heatmap_data[barrier] = (\n",
        "            df.loc[mask]\n",
        "              .groupby('intent')['sentiment_score']\n",
        "              .mean()\n",
        "              .reindex([\"seek\",\"considering\",\"ambivalent\",\"delayed\",\"avoid\"])\n",
        "        )\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(pd.DataFrame(heatmap_data).T, annot=True, cmap=\"RdYlGn\", center=0)\n",
        "plt.title(\"Sentiment by Intent and Barrier Type\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/sentiment_barrier_heatmap.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bi8HE0h4u7dL",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bi8HE0h4u7dL"
      },
      "outputs": [],
      "source": [
        "# Causal Findings\n",
        "panel = (\n",
        "    df.groupby(['subreddit', 'month'])\n",
        "      .agg(pct_avoid=('intent', lambda x: (x=='avoid').mean()),\n",
        "           pct_seek=('intent', lambda x: (x=='seek').mean()),\n",
        "           avg_sentiment=('sentiment_score','mean'),\n",
        "           barrier_rate=('has_any_barrier','mean'))\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=panel, x='barrier_rate', y='pct_avoid', hue='subreddit')\n",
        "plt.title(\"Barrier Rate vs Avoidance Intent by Subreddit\")\n",
        "plt.xlabel(\"Barrier Rate (Proportion of Posts)\")\n",
        "plt.ylabel(\"Avoidance Intent Rate\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/barrier_vs_avoidance_scatter.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AGhMJs-xuwsP",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AGhMJs-xuwsP"
      },
      "outputs": [],
      "source": [
        "# Narrative Interpretation\n",
        "print(\"\\nðŸ©º INTERPRETATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"1ï¸âƒ£ Cost and bureaucratic barriers show the strongest co-occurrence with 'avoid' intent.\")\n",
        "print(\"2ï¸âƒ£ Pregnancy-related subreddits (BabyBumps, pregnant, GestationalDiabetes) show higher emotional tone and more stigma mentions.\")\n",
        "print(\"3ï¸âƒ£ Chronic condition subreddits (diabetes_t1/t2) show steadier sentiment but higher cost discussions.\")\n",
        "print(\"4ï¸âƒ£ Negative sentiment correlates moderately with avoidance intent across all communities.\")\n",
        "print(\"5ï¸âƒ£ Overall, cost + stigma barriers appear to have the largest predictive effect on care-seeking delay or avoidance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1raRRQQyqgg",
      "metadata": {
        "id": "I1raRRQQyqgg"
      },
      "outputs": [],
      "source": [
        "# ----- LDA Topic Visualization (Bar Chart of Top Words per Topic) -----\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "n_top_words = 8   # choose how many per topic\n",
        "num_topics = lda.n_components\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=num_topics // 2 + num_topics % 2,\n",
        "    ncols=2,\n",
        "    figsize=(14, num_topics * 2.2),\n",
        "    constrained_layout=True\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, topic in enumerate(lda.components_[:num_topics]):\n",
        "    top_indices = topic.argsort()[::-1][:n_top_words]\n",
        "    words = [vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
        "    weights = topic[top_indices]\n",
        "\n",
        "    axes[idx].barh(words[::-1], weights[::-1], color=\"#7b9acc\")\n",
        "    axes[idx].set_title(f\"Topic {idx}\")\n",
        "    axes[idx].invert_yaxis()\n",
        "\n",
        "# Turn off unused subplots\n",
        "for ax in axes[num_topics:]:\n",
        "    ax.set_visible(False)\n",
        "\n",
        "plt.suptitle(\"LDA Topics â€“ Top Keywords\", fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TrrKPz4R7ii_",
      "metadata": {
        "id": "TrrKPz4R7ii_"
      },
      "outputs": [],
      "source": [
        "# Barchart BERTopics\n",
        "fig_sizes = topic_model.visualize_barchart(top_n_topics=20)\n",
        "fig_sizes.show()\n",
        "fig_sizes.write_html(\"figures/bertopic_topic_sizes.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m0pqgS_v8dt5",
      "metadata": {
        "id": "m0pqgS_v8dt5"
      },
      "outputs": [],
      "source": [
        "# --- LDA topic counts ---\n",
        "lda_counts = df[\"lda_topic\"].value_counts().sort_index()\n",
        "lda_df = lda_counts.rename_axis(\"topic\").reset_index(name=\"count\")\n",
        "lda_df[\"model\"] = \"LDA\"\n",
        "\n",
        "# --- BERTopic topic counts (exclude -1 = outliers) ---\n",
        "bt_counts = df[df[\"topic\"] != -1][\"topic\"].value_counts().sort_index()\n",
        "bt_df = bt_counts.rename_axis(\"topic\").reset_index(name=\"count\")\n",
        "bt_df[\"model\"] = \"BERTopic\"\n",
        "\n",
        "# Combine\n",
        "topic_sizes = pd.concat([lda_df, bt_df], ignore_index=True)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "\n",
        "# LDA\n",
        "axes[0].bar(lda_df[\"topic\"].astype(str), lda_df[\"count\"])\n",
        "axes[0].set_title(\"LDA Topic Sizes\")\n",
        "axes[0].set_xlabel(\"LDA Topic\")\n",
        "axes[0].set_ylabel(\"Number of Posts\")\n",
        "\n",
        "# BERTopic\n",
        "axes[1].bar(bt_df[\"topic\"].astype(str), bt_df[\"count\"])\n",
        "axes[1].set_title(\"BERTopic Topic Sizes\")\n",
        "axes[1].set_xlabel(\"BERTopic Topic\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OjiqqSwDxKpS",
      "metadata": {
        "id": "OjiqqSwDxKpS"
      },
      "source": [
        "# Causal Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z2E5HvmRNi4H",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z2E5HvmRNi4H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "# Set datetime\n",
        "df['created'] = pd.to_datetime(df['created'])\n",
        "\n",
        "# Define subreddit groups\n",
        "groups = {\n",
        "    \"pregnancy\": [\n",
        "        \"BabyBumps\", \"pregnant\", \"pregnantover35\", \"GestationalDiabetes\"\n",
        "    ],\n",
        "    \"diabetes\": [\n",
        "        \"diabetes\", \"diabetes_t1\", \"diabetes_t2\"\n",
        "    ],\n",
        "    \"access_insurance\": [\n",
        "        \"AskDocs\", \"HealthInsurance\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Helper: create monthly panel for a group\n",
        "def make_group_panel(sub_list, label):\n",
        "    tmp = df[df['subreddit'].isin(sub_list)].copy()\n",
        "    if tmp.empty:\n",
        "        print(f\"[WARN] No posts found for group: {label}\")\n",
        "        return None\n",
        "\n",
        "    tmp['month'] = tmp['created'].dt.to_period('M').dt.to_timestamp()  # month start\n",
        "\n",
        "    panel = (\n",
        "        tmp.groupby('month')\n",
        "           .agg(\n",
        "               barrier_rate=('has_any_barrier', 'mean'),\n",
        "               cost_rate=('has_cost', 'mean'),\n",
        "               avoid_rate=('intent', lambda x: (x == 'avoid').mean()),\n",
        "               seek_rate=('intent', lambda x: (x == 'seek').mean()),\n",
        "               avg_sentiment=('sentiment_score', 'mean'),\n",
        "               n_posts=('id', 'count')\n",
        "           )\n",
        "           .sort_index()\n",
        "    )\n",
        "    panel['group'] = label\n",
        "    return panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AAr8bHy5NccM",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AAr8bHy5NccM"
      },
      "outputs": [],
      "source": [
        "# Build panels for all three groups\n",
        "panels = {}\n",
        "for label, subs in groups.items():\n",
        "    panel = make_group_panel(subs, label)\n",
        "    if panel is not None:\n",
        "        # Regularize to monthly frequency and interpolate small gaps\n",
        "        panel = panel.asfreq('MS').interpolate()\n",
        "        panels[label] = panel\n",
        "        print(f\"{label}: {len(panel)} monthly observations\")\n",
        "\n",
        "# Helper: Granger wrapper\n",
        "def run_granger(ts_df, x_col, y_col, maxlag=3, title=\"\"):\n",
        "    \"\"\"\n",
        "    Test whether x_col Granger-causes y_col in ts_df.\n",
        "    ts_df must be indexed by time and contain columns [x_col, y_col].\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"GRANGER CAUSALITY TEST: {x_col} â†’ {y_col} {title}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data = ts_df[[y_col, x_col]].dropna()  # order: [y, x]\n",
        "    if len(data) < (maxlag*2 + 1):\n",
        "        print(f\"Not enough data points for maxlag={maxlag}. Only {len(data)} rows.\")\n",
        "        return\n",
        "\n",
        "    results = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n",
        "    for lag in range(1, maxlag+1):\n",
        "        p_val = results[lag][0]['ssr_ftest'][1]\n",
        "        print(f\"Lag {lag}: p-value = {p_val:.4f}\")\n",
        "    print(\"Note: p-values < 0.05 suggest x helps predict future y (Granger-causal).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uC__iejNNVhp",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uC__iejNNVhp"
      },
      "outputs": [],
      "source": [
        "# Run Granger for each group and predictor\n",
        "maxlag = 3\n",
        "for label, panel in panels.items():\n",
        "    # Any barrier -> avoidance\n",
        "    run_granger(\n",
        "        panel,\n",
        "        x_col=\"barrier_rate\",\n",
        "        y_col=\"avoid_rate\",\n",
        "        maxlag=maxlag,\n",
        "        title=f\"({label} communities)\"\n",
        "    )\n",
        "\n",
        "    # Cost barrier -> avoidance\n",
        "    run_granger(\n",
        "        panel,\n",
        "        x_col=\"cost_rate\",\n",
        "        y_col=\"avoid_rate\",\n",
        "        maxlag=maxlag,\n",
        "        title=f\"({label} communities, cost barriers)\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r3gA8R8afn9W",
      "metadata": {
        "id": "r3gA8R8afn9W"
      },
      "source": [
        "# Causal Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uucLtSBdfyaO",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uucLtSBdfyaO"
      },
      "outputs": [],
      "source": [
        "# Causal Visualizations: Barriers vs Avoidance Over Time\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "# Combine all group panels into a single DataFrame\n",
        "combined_panel = []\n",
        "\n",
        "for label, panel in panels.items():\n",
        "    tmp = panel.copy()\n",
        "    tmp = tmp.reset_index().rename(columns={\"month\": \"date\"})\n",
        "    tmp[\"group\"] = label\n",
        "    combined_panel.append(tmp)\n",
        "\n",
        "combined_panel = pd.concat(combined_panel, ignore_index=True)\n",
        "combined_panel.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QgHUKnQtf-yj",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QgHUKnQtf-yj"
      },
      "outputs": [],
      "source": [
        "# Line plot: barrier_rate and avoid_rate by group over time\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(\n",
        "    data=combined_panel,\n",
        "    x=\"date\", y=\"barrier_rate\",\n",
        "    hue=\"group\", marker=\"o\"\n",
        ")\n",
        "plt.title(\"Barrier Rate Over Time by Community Group\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Barrier Rate (proportion of posts)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/barrier_rate_over_time_by_group.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(\n",
        "    data=combined_panel,\n",
        "    x=\"date\", y=\"avoid_rate\",\n",
        "    hue=\"group\", marker=\"o\"\n",
        ")\n",
        "plt.title(\"Avoidance Intent Over Time by Community Group\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Avoidance Rate (proportion of posts)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/avoid_rate_over_time_by_group.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ll9ZLGtGgVTU",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ll9ZLGtGgVTU"
      },
      "outputs": [],
      "source": [
        "# FacetGrid: barrier_rate and avoid_rate over time, per group\n",
        "# Melt to long format to plot both series in one facet per group\n",
        "long_panel = combined_panel.melt(\n",
        "    id_vars=[\"date\", \"group\"],\n",
        "    value_vars=[\"barrier_rate\", \"avoid_rate\"],\n",
        "    var_name=\"metric\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "\n",
        "g = sns.relplot(\n",
        "    data=long_panel,\n",
        "    x=\"date\", y=\"value\",\n",
        "    hue=\"metric\",\n",
        "    col=\"group\",\n",
        "    kind=\"line\",\n",
        "    marker=\"o\",\n",
        "    facet_kws={\"sharey\": False, \"sharex\": True},\n",
        "    height=4, aspect=1.2\n",
        ")\n",
        "g.set_titles(\"{col_name} communities\")\n",
        "g.set_xlabels(\"Month\")\n",
        "g.set_ylabels(\"Rate\")\n",
        "for ax in g.axes.flatten():\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "        label.set_ha(\"right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/barrier_vs_avoid_facet_by_group.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qS8LzDzgz92B",
      "metadata": {
        "id": "qS8LzDzgz92B"
      },
      "source": [
        "# Sample Posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IXxlQ9aw0NIH",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IXxlQ9aw0NIH"
      },
      "outputs": [],
      "source": [
        "# Function to anonymize & shorten text\n",
        "def anonymize_sample(text, max_len=250):\n",
        "    text = str(text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Shorten long posts\n",
        "    if len(text) > max_len:\n",
        "        text = text[:max_len] + \"...\"\n",
        "    return text\n",
        "\n",
        "# Select posts that actually HAVE content + a barrier or intent\n",
        "sample_df = df[\n",
        "    (df[\"text_clean\"].str.len() > 80) &\n",
        "    (df[\"barriers\"].apply(len) > 0) |\n",
        "    (df[\"intent\"].notna())\n",
        "]\n",
        "\n",
        "# Sample N posts\n",
        "N = 5\n",
        "samples = sample_df.sample(N, random_state=42)\n",
        "\n",
        "# Create anonymized output\n",
        "samples_display = samples[[\n",
        "    \"subreddit\", \"barriers\", \"intent\", \"sentiment_label\", \"text_clean\"\n",
        "]].copy()\n",
        "\n",
        "samples_display[\"excerpt\"] = samples_display[\"text_clean\"].apply(anonymize_sample)\n",
        "\n",
        "# Show final output for screenshots\n",
        "samples_display = samples_display[[\n",
        "    \"subreddit\", \"barriers\", \"intent\", \"sentiment_label\", \"excerpt\"\n",
        "]]\n",
        "\n",
        "samples_display"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
